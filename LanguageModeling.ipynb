{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code to download dataset directly from URL\n",
    "\n",
    "#for c in ['sports', 'religion', 'local', 'international', 'economy', 'culture']:\n",
    "#    r = requests.get(\n",
    "#        'https://sourceforge.net/projects/kalimat/files/kalimat/document-collection/articles-'+c+'.zip/download')\n",
    "#    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "#    z.extractall('./newData/'+c)\n",
    "\n",
    "# ----replace \"Data\" with \"newData\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "* dataset= https://sourceforge.net/projects/kalimat/files/kalimat/document-collection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#for performance measure:\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import util as ut\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_text1 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون والاذاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "err_text1 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون و الاشاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "\n",
    "test_text2 = \"حقق منتخبنا الوطني الاول لكرة القدم كل اهدافه المطلوبة\"\n",
    "err_text2 = \"حقق منتخبنا الوطني الاول لكرة القسم كل اهدافه المطلوبة\"\n",
    "err_text2 = \"منتخبنا يتألق ويهم الشباك\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add cleaning, lemmatizing + removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "#! pip install qalsadi\n",
    "import qalsadi.lemmatizer\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "def lemmatize(text):\n",
    "    t=lemmer.lemmatize_text(text)\n",
    "    s = ut.stringify(t)\n",
    "    return s\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sortir de la boucle si on retrouve un ngram tres proche (inversion/suppression/ajout d'un caratere) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return ngram that don't exist in corpustoken or exist with minor change (typography mistake)\n",
    "def OneSubstitution(word, word2):# return true if exists one and only onesubstituion error\n",
    "    if len(word)==len(word2):\n",
    "        l = []\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == word2[i]:\n",
    "                l.append(0)\n",
    "            else:\n",
    "                l.append(1)\n",
    "        return (sum(l) == 1)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "\n",
    "def OneInversion(word, word2):\n",
    "    if len(word) == len(word2):  # returns true if exists one and only inversion error\n",
    "        l1 = []\n",
    "        l2 = []\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == word2[i]:\n",
    "                l1.append(0)\n",
    "            else:\n",
    "                l1.append(1)\n",
    "                l2.append(word[i])\n",
    "                l2.append(word2[i])\n",
    "        if sum(l1) == 2:\n",
    "          return(l2[0] == l2[3] and l2[1] == l2[2])\n",
    "        else:\n",
    "            return(False)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "\n",
    "def AddSubErr(word1, word2):  # ajout/suppression d'un seul caracter\n",
    "    w1 = set(word1)\n",
    "    w2 = set(word2)\n",
    "    maxi=max(len(w1-w2),len(w2-w1))\n",
    "    if maxi != 1:\n",
    "        return(False)\n",
    "    else :\n",
    "        if len(w1)-len(w2) == 1:\n",
    "            lw1=list(word1)\n",
    "            lw1.remove(''.join(w1-w2))\n",
    "            s = ''.join(str(x) for x in lw1)\n",
    "            return(s==word2)\n",
    "        elif len(w1)-len(w2) == -1:\n",
    "            lw2 = list(word2)\n",
    "            lw2.remove(''.join(w2-w1))\n",
    "            s = ''.join(str(x) for x in lw2)\n",
    "            return(s == word1)\n",
    "        else:\n",
    "            return(False)\n",
    "def lookslike(words, corpustoken):\n",
    "    for word in words:\n",
    "        for c in corpustoken:\n",
    "            if AddSubErr(word,c): \n",
    "                return(word)\n",
    "            elif OneInversion(word,c) or OneSubstitution(word,c) : #inversion / substitution\n",
    "                return(word)\n",
    "    return('')\n",
    "\n",
    "def CheckNGramInCorpus(texttoken, corpustoken):\n",
    "    ngram = [g for g in texttoken if g not in corpustoken]\n",
    "    return (ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WrongNGram(text, n=2):\n",
    "    text = ut.very_clean(text)\n",
    "    #text = lemmatize(text)\n",
    "    token = ut.get_all_ngrams(text, nrange=n)\n",
    "    ngram_i = []\n",
    "    err = ''\n",
    "    for directory in os.listdir(\"../Data\"):\n",
    "        for filename in os.listdir(\"../Data/\"+directory):\n",
    "            f = open(\"../Data/\" + directory+\"/\"+filename, \"r\", encoding=\"utf8\")\n",
    "            g = ut.very_clean(f.read().replace('\\n', ' '))\n",
    "            #g = lemmatize(g)\n",
    "            ngram_i = ut.get_all_ngrams(g, nrange=n)\n",
    "            #print(token)\n",
    "            #print(ngram_i)\n",
    "            token = CheckNGramInCorpus(token, ngram_i)\n",
    "            err = lookslike(token,ngram_i)\n",
    "            #if len(err) !=0 :\n",
    "                #print(\"potential error in: \",err) \n",
    "            if len(token) == 0:\n",
    "                #print(token ,'no error')\n",
    "                return(token)\n",
    "            f.close()\n",
    "    #print(\"potential error in :\", err)\n",
    "    #print(\"error in : \\n \",token)\n",
    "    return(token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mispelled_word (text,n=2): #sans considerer lemmatizationb\n",
    "    l = WrongNGram(text)\n",
    "    #print(l)\n",
    "    if len(l)>1:\n",
    "        l2 = l[0].split(\"_\")\n",
    "        #print(\"mispelled word is : \")\n",
    "        return(l2[-1])\n",
    "    elif len(l) == 1 : #mistake in first or last word\n",
    "        #print(\"the error is either first or last word\")\n",
    "        return(l)\n",
    "    else :\n",
    "        #print(\"no error in this sentence\")\n",
    "        return(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ويهم'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mispelled_word(err_text2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../testdata.csv\", dtype='string')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y vs y_predit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [ c for c in df.sentence]\n",
    "y_ngram_pred = [mispelled_word(c) for c in test]\n",
    "y_pred = []\n",
    "for i in y_ngram_pred:\n",
    "    if type(i) == str:\n",
    "        y_pred.append(i)\n",
    "    else:\n",
    "        y_pred.append(i[0])\n",
    "y = [y.lstrip().rstrip() for y in df.wrong_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y)\n",
    "#print(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* precision & recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, y_pred):\n",
    "    c = 0  # nbre de mots correctement étiquetés\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == y_pred[i]:\n",
    "            c += 1\n",
    "\n",
    "    p = c / len(y)\n",
    "    return(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language modelisong precision =  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "prec = precision(y, y_pred)\n",
    "print(\"language modelisong precision = \",prec)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
