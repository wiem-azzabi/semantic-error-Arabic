{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aravec Model Word Embedding\n",
    "* github model source link : https://github.com/bakrianoo/aravec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDX5kzUJJp8B"
   },
   "source": [
    "## Install/Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re ##regularexpression\n",
    "import spacy\n",
    "import nltk\n",
    "import util as ut\n",
    "#for performance measure: \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils  # utility fnc for pickling, common scipy operations etc\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./spacy.aravec.model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing Class\n",
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer, **cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        preprocessed = ut.clean_str(text)\n",
    "        return self.tokenizer(preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the `Preprocessor` Class\n",
    "nlp.tokenizer = Preprocessor(nlp.tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"طبخ الطباخ المرطبات\"\n",
    "text11 = \"طبخ الطباخ المركبات\"\n",
    "text2 = \"حقق منتخبنا الوطني الاول لكرة القدم كل اهدافه المطلوبة\"\n",
    "text22 = \"حقق منتخبنا الوطني الاول لكرة القسم كل اهدافه المطلوبة\"\n",
    "text3 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون والاذاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "text33 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون و الاشاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "text4 = \"منتخبنا يتألق ويهم الشباك\"\n",
    "text44 = \"منتخبنا يتألق ويهز الشباك\"\n",
    "text5 =\"اصطاد الصياد سمكة\"\n",
    "text55 =\"اصطاد الصياد سكة\"\n",
    "text6 =\"لكن قد لا يعرف الكثيرون أن موهبتها التمثيلية وجدت طريقها إلى عالم الفن بالصدفة البحته\"\n",
    "text66 = \"لكن قد لا يعرف الكثيرون أن موهبتها التمثيلية وجدت طريقها إلى عالم الفن بالصدفة البحته\"\n",
    "text7 = \"شهد السباق تفاعلا وحضورا كبيرا من محبي هذه الرياضة الاصيلة كونها تمثل رمزا للاصالة العربية ونهجا على طريق الاجداد\"\n",
    "text77 = \"شهد السباق تفاعلا وحضورا كبيرا من محبي هذه الرياضة الاصيلة كونها تمثل رمزا للاصالة العبرية ونهجا على طريق الاجداد\"\n",
    "text8=\"\"\n",
    "text = text77\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Least Similar vector\n",
    "* odd word is the one that is the least similar to all other words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score de chaque mot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'المركبات'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def min_max_similarity(text):\n",
    "    origin = nltk.word_tokenize((ut.very_clean(text)))  # preprocessing\n",
    "    text = nltk.word_tokenize(ut.lemmatize(ut.very_clean(text))) #lemmatizing\n",
    "    token = [nlp(x) for x in text] #embedding\n",
    "    avg_list=[]\n",
    "    for i in range(len(token)): \n",
    "        sim_list=[]\n",
    "        for j in range(len(token)):\n",
    "            sim_list.append(token[i].similarity(token[j]))\n",
    "        avg_list.append(np.mean(sim_list))\n",
    "        #print(token[i], \" = \", avg_list[i])\n",
    "    print(\"score de chaque mot\")\n",
    "    return(origin[avg_list.index(min(avg_list))])\n",
    "min_max_similarity(text11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Furthest Vector\n",
    "* odd word is the word whose vector is the furthest(considering cosine similarity as distance) from the context vector of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'المركبات'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Un mot est considéré erroné si son vecteur est éloigné \n",
    "# (par rapport à un seuil) du vecteur moyenne/somme de son contexte \n",
    "# context ~ (vecteur moyenne des autres mots)\n",
    "\n",
    "def context_vec(token, word):\n",
    "    avg = np.mean([nlp(w).vector for w in token if w != word],\n",
    "                  axis=0)  # avg euclidean distance\n",
    "    return avg\n",
    "\n",
    "#-----------------------\n",
    "#distance(similarity) between two vectors\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity=0\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "    return cosine_similarity\n",
    "#---------------------------------------\n",
    "\n",
    "\n",
    "def Aravec_odd_word(text):\n",
    "    origin = nltk.word_tokenize((ut.very_clean(text))) #preprocessing\n",
    "    text = nltk.word_tokenize(ut.lemmatize(ut.very_clean(text))) #lemmatizing\n",
    "    sptext = [nlp(x) for x in text] #embedding\n",
    "    l = list([])\n",
    "    for word in sptext:\n",
    "        context = context_vec(sptext, word)\n",
    "        #l = [math.dist(nlp(w).vector,context) for w in token]\n",
    "        l.append(similarity_cosine(nlp(word).vector, context))\n",
    "    farthestword = origin[l.index(min(l))]\n",
    "    return(farthestword)\n",
    "\n",
    "\n",
    "Aravec_odd_word(text11)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wrong_word</th>\n",
       "      <th>correct_word</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اصطاد الصياد سكة</td>\n",
       "      <td>السكة</td>\n",
       "      <td>السمكة</td>\n",
       "      <td>deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حقق منتخبنا الوطني الاول لكرة القسم كل اهدافه ...</td>\n",
       "      <td>القسم</td>\n",
       "      <td>القدم</td>\n",
       "      <td>substitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تنطلق اليوم الدورة البرامجية الجديدة للتليفزيو...</td>\n",
       "      <td>الاشاعة</td>\n",
       "      <td>الاذاعة</td>\n",
       "      <td>substitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>طبخ الطباخ المركبات</td>\n",
       "      <td>المركبات</td>\n",
       "      <td>المرطبات</td>\n",
       "      <td>substitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شهد السباق تفاعلا وحضورا كبيرا من محبي هذه الر...</td>\n",
       "      <td>العبرية</td>\n",
       "      <td>العربية</td>\n",
       "      <td>inversion error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>انه سعيد بسيناريو الفيلم الذي يلخص الواسع المص...</td>\n",
       "      <td>الواسع</td>\n",
       "      <td>الواقع</td>\n",
       "      <td>substitution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>يقوم الدكتور بعلاج مرضى كثيرين يعانقون من امرا...</td>\n",
       "      <td>يعانقون</td>\n",
       "      <td>يعانون</td>\n",
       "      <td>adding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>أعلن المجلس الوطني عن نتائج المسابقة في سياق ف...</td>\n",
       "      <td>عرش</td>\n",
       "      <td>عشر</td>\n",
       "      <td>inversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>تعتبر الثقافة البصرية من ارقى واقدم انواع الثق...</td>\n",
       "      <td>البرية</td>\n",
       "      <td>البشرية</td>\n",
       "      <td>deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>اللام لا يصنع السعادة</td>\n",
       "      <td>اللام</td>\n",
       "      <td>المال</td>\n",
       "      <td>inversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>الفلاح يقصد القمح</td>\n",
       "      <td>يقصد</td>\n",
       "      <td>يحصد</td>\n",
       "      <td>substitution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence wrong_word correct_word  \\\n",
       "0                                    اصطاد الصياد سكة      السكة       السمكة   \n",
       "1   حقق منتخبنا الوطني الاول لكرة القسم كل اهدافه ...     القسم         القدم   \n",
       "2   تنطلق اليوم الدورة البرامجية الجديدة للتليفزيو...   الاشاعة       الاذاعة   \n",
       "3                                 طبخ الطباخ المركبات   المركبات     المرطبات   \n",
       "4   شهد السباق تفاعلا وحضورا كبيرا من محبي هذه الر...   العبرية       العربية   \n",
       "5   انه سعيد بسيناريو الفيلم الذي يلخص الواسع المص...     الواسع       الواقع   \n",
       "6   يقوم الدكتور بعلاج مرضى كثيرين يعانقون من امرا...    يعانقون       يعانون   \n",
       "7   أعلن المجلس الوطني عن نتائج المسابقة في سياق ف...        عرش          عشر   \n",
       "8   تعتبر الثقافة البصرية من ارقى واقدم انواع الثق...     البرية      البشرية   \n",
       "9                              اللام لا يصنع السعادة       اللام        المال   \n",
       "10                                  الفلاح يقصد القمح       يقصد         يحصد   \n",
       "\n",
       "         error_type  \n",
       "0          deletion  \n",
       "1      substitution  \n",
       "2      substitution  \n",
       "3      substitution  \n",
       "4   inversion error  \n",
       "5      substitution  \n",
       "6            adding  \n",
       "7         inversion  \n",
       "8          deletion  \n",
       "9         inversion  \n",
       "10     substitution  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../testdata.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y vs y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [c for c in df.sentence]\n",
    "y_aravec_pred = [Aravec_odd_word(c) for c in test]\n",
    "y_pred = []\n",
    "for i in y_aravec_pred:\n",
    "    if type(i) == str:\n",
    "        y_pred.append(i)\n",
    "    else:\n",
    "        y_pred.append(i[0])\n",
    "y = [y.lstrip().rstrip() for y in df.wrong_word]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aravec model precision =  0.36363636363636365\n"
     ]
    }
   ],
   "source": [
    "precision = ut.precision(y, y_pred)\n",
    "print(\"aravec model precision = \",precision)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
